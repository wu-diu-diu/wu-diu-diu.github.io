<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><meta name="author" content="武丢丢"><meta name="renderer" content="webkit"><meta name="copyright" content="武丢丢"><meta name="keywords" content="武丢丢"><meta name="description" content="null"><meta name="Cache-Control" content="no-cache"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><title>transformer-MLP · Punchy's Blog</title><link rel="stylesheet" href="/css/style.css?v=2018.7.9"><link rel="stylesheet" href="/css/animation.css?v=2018.7.9"><link rel="icon" href="/img/assets/favicon.ico"><link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css?version=1.5.6"><!-- scripts--><script>(function( w ){
  "use strict";
  // rel=preload support test
  if( !w.loadCSS ){
    w.loadCSS = function(){};
  }
  // define on the loadCSS obj
  var rp = loadCSS.relpreload = {};
  // rel=preload feature support test
  // runs once and returns a function for compat purposes
  rp.support = (function(){
    var ret;
    try {
      ret = w.document.createElement( "link" ).relList.supports( "preload" );
    } catch (e) {
      ret = false;
    }
    return function(){
      return ret;
    };
  })();

  // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
  // then change that media back to its intended value on load
  rp.bindMediaToggle = function( link ){
    // remember existing media attr for ultimate state, or default to 'all'
    var finalMedia = link.media || "all";

    function enableStylesheet(){
      link.media = finalMedia;
    }

    // bind load handlers to enable media
    if( link.addEventListener ){
      link.addEventListener( "load", enableStylesheet );
    } else if( link.attachEvent ){
      link.attachEvent( "onload", enableStylesheet );
    }

    // Set rel and non-applicable media type to start an async request
    // note: timeout allows this to happen async to let rendering continue in IE
    setTimeout(function(){
      link.rel = "stylesheet";
      link.media = "only x";
    });
    // also enable media after 3 seconds,
    // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
    setTimeout( enableStylesheet, 3000 );
  };

  // loop through link elements in DOM
  rp.poly = function(){
    // double check this to prevent external calls from running
    if( rp.support() ){
      return;
    }
    var links = w.document.getElementsByTagName( "link" );
    for( var i = 0; i < links.length; i++ ){
      var link = links[ i ];
      // qualify links to those with rel=preload and as=style attrs
      if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
        // prevent rerunning on link
        link.setAttribute( "data-loadcss", true );
        // bind listeners to toggle media back
        rp.bindMediaToggle( link );
      }
    }
  };

  // if unsupported, run the polyfill
  if( !rp.support() ){
    // run once at least
    rp.poly();

    // rerun poly on an interval until onload
    var run = w.setInterval( rp.poly, 500 );
    if( w.addEventListener ){
      w.addEventListener( "load", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    } else if( w.attachEvent ){
      w.attachEvent( "onload", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    }
  }


  // commonjs
  if( typeof exports !== "undefined" ){
    exports.loadCSS = loadCSS;
  }
  else {
    w.loadCSS = loadCSS;
  }
}( typeof global !== "undefined" ? global : this ) );</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" defer></script><script src="/js/main.js?v=2018.7.9" defer></script><!-- fancybox--><link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script><!-- busuanzi--><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><meta name="generator" content="Hexo 7.1.1"></head><body><section class="profile-close" id="cxo-profile"><div class="profile-avatar"><i class="fa fa-caret-left"></i><img src="/img/assets/me.png"></div><!--.profile-saying
  i.fa.fa-comment
  .saying--><div class="cxo-profile-inner"><div class="profile-name">punchy</div><div class="profile-signature">Stay foolish Stay hungry</div><div class="read-progress"></div></div></section><header id="cxo-intro" style="height: 70vh;background-image: url(/img/intro/index-bg.png);"><nav id="cxo-intro-nav"><section><div class="intro-nav-title"><a href="/">Wudiudiu's Blog</a></div><div class="intro-nav-label-box"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div><i class="fa fa-bars intro-nav-menu"><div class="intro-nav-drop"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div></i><div class="clear"></div></section></nav><h1 class="post-title">transformer-MLP</h1><div class="post-intros"><div class="post-intro-meta"><span class="post-intro-time"><i class="post-intro-calendar fa fa-calendar"></i><span>2025-04-07</span></span><span class="post-intro-tags"><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="LLM"> LLM</a></span></div></div></header><article class="cxo-up" id="cxo-content-outer"><section id="cxo-content-inner"><article class="article-entry" id="post"><script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    processEscapes: true,
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

<h1 id="为什么会有MLP？"><a href="#为什么会有MLP？" class="headerlink" title="为什么会有MLP？"></a>为什么会有MLP？</h1><p>transformer架构中，最主要的两个模块就是Attention-Layer和MLP-Layer。MLP全称为多层感知机，是神经网络的最主要的组成部分。其结构十分简单，主要由两层线性变换层和一个激活函数构成，其公式如下：</p>
<p>$$<br>MLP &#x3D; \sigma(X \cdot W_1 + b_1) \cdot W_2 + b_2 \tag{1.1}<br>$$</p>
<p>在神经网络中，上述公式是构成整个模型基础，一个线性变换层则包括了上面的权重和偏置，通过引入激活函数来引入非线性，从而提高模型的非线性拟合能力。</p>
<p>既然MLP在神经网络中是通过引入非线性变换来提高模型的拟合高维空间的能力，那么在transformer中，MLP也是这样的作用，即：</p>
<ul>
<li>通过线性变换层和激活函数，MLP能够将注意力机制的输出token进行非线性变换，从而捕捉token更复杂的语义，增强模型的表达能力。</li>
<li>token在经过attention层之后，其语义向量是涵盖了上下文信息的，MLP的作用就是进一步的去处理token的语义向量，帮助模型更好的理解向量中包含的上下文信息。这也是为什么transformer中Attention层之后就必须借一个MLP层，二者交替相接，重复多次，构成了模型的最重要的组成部分。</li>
</ul>
<h1 id="MLP的架构"><a href="#MLP的架构" class="headerlink" title="MLP的架构"></a>MLP的架构</h1><p>而在transformer的MLP中，上不同的模型可能会在这个基础上添加一些dropout层来防止过拟合，或者添加gate层(门控投影)来进行一种门控操作。</p>
<p>下面以Llama的一个Base模型(0.8B)的MLP层为例，其内部结构如下图：</p>
<div style="text-align: center;">
  <style>.hhkspvzwotdg{width: 40%; height: 40%;}</style><img src="/2025/04/07/transformer-MLP/01.png" class="hhkspvzwotdg" alt="MLP层">
</div>

<p>由上图可知，大小为<strong>H:4096</strong>的隐藏向量在进入MLP层时，会并行进入gate层和up_proj层，得到一个大小为<strong>intermediate_size:11008</strong>的中间向量，然后gate的输出经过ACT2FN，这是一个包含不同激活函数的字典，llama中使用的激活函数是silu，其函数图像如下：</p>
<div style="text-align: center;">
  <style>.nxpqnbwxqrse{width: 40%; height: 40%;}</style><img src="/2025/04/07/transformer-MLP/02.png" class="nxpqnbwxqrse" alt="Silu激活函数">
</div>

<p>可以看到和relu函数很像，不同之处在于函数在过零点是更平滑一点，则零点附近的梯度不是像relu一样非零即1，则其同时具有relu的优点：避免梯度饱和区带来的可能的梯度消失。又避免了relu的缺点:零点梯度不存在且附近的梯度不够平滑。</p>
<p>经过激活函数后，gate层的输出和up_proj层的输出<strong>逐元素相乘</strong>，这样做的目的是可以通过gate层来控制up_proj层输出的高维向量的各个维度的值的大小，相当于乘以一个权重。这种操作可以让模型动态的调整MLP中高维向量的各个维度的权重，自主的选择哪些维度是重要的，哪些维度是不重要的。因为对于某些语义比较简单的token，使用一个11008维的向量来表达会过于冗余，过于复杂，这是如果模型能通过gate来抑制这个11008维向量的某些维度来表征这个简单语义的token，这样就可以降低模型的复杂度。</p>
<p>最后再经过一个down_proj将向量映射到低维。总的来说，MLP就是将token映射到高维，引入非线性，引入权重，在高维空间中捕捉token更复杂的语义信息后，再将token压缩至低维。</p>
<h1 id="MoE"><a href="#MoE" class="headerlink" title="MoE"></a>MoE</h1><h2 id="MoE的简史"><a href="#MoE的简史" class="headerlink" title="MoE的简史"></a>MoE的简史</h2><p>MOE全称为：Mixture of Experts, 混合专家架构。随着LLM的发展，模型变得越来越大，参数变得越来越多，这对预训练带来了很多挑战，比如训练时间，比如显存占用。很多模型一张最好的GPU显卡都是放不下的，只能将模型切分，这同时会带来通信的问题。所以，MoE应运而生。</p>
<p>MoE的思想来源于集成学习，集成学习是通过训练多个模型即基学习器来解决同一个问题，将多个模型的预测结果简单的组合(投票或者平均)。集成学习的主要目标是通过减少过拟合，提高泛化能力，以提高预测性能。集成学习在训练过程中，利用训练数据集训练基学习器，基学习器的算法可以是决策树、SVM、线性回归、KNN等，在推理过程中对于输入的X，在每个基学习器得到相应的答案后将所有结果有机统一起来，例如通过求均值的方法解决数值类问题，通过投票方式解决分类问题。</p>
<p>MoE的的想法是，每个网络即专家，处理训练样本的不同子集，每个网络专注于输入空间的特定区域。那么如何选择哪个专家来处理哪个区域呢？这就是门控网络要决定的。</p>
<h2 id="MoE的构成"><a href="#MoE的构成" class="headerlink" title="MoE的构成"></a>MoE的构成</h2><p>MoE其实是对原本transformer中的MLP的替代，这里我们称原始的MLP为稠密的，因为MoE是稀疏的。为什么稀疏？看一下它的架构就知道了。如下图所示：</p>
<div style="text-align: center;">
  <style>.kjeibygvijfh{width: 60%; height: auto;}</style><img src="/2025/04/07/transformer-MLP/03.png" class="kjeibygvijfh" alt="MoE">
</div>

<p>可以看到，一个门控网络Router的输入是token的语义向量，输出是一个概率。这个概率决定了这个token会被哪个FFN所处理。这就是MoE的实现办法。而FFN其实和MLP是一样的只是叫法不同，其代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim_vector, dim_hidden, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.feedforward = nn.Sequential(</span><br><span class="line">            nn.Linear(dim_vector, dim_hidden),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(dropout),</span><br><span class="line">            nn.Linear(dim_hidden, dim_vector)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = self.feedforward(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<p>门控网络是怎么实现的呢？其公式如下所示：</p>
<p>$$<br>G_{\sigma}(x) &#x3D; Softmax(x \cdot W_g) \tag{1.2}<br>$$</p>
<p>就是输入经过一个线性变换后，再使用softmax将向量概率化。以TopK举例，假如有四个专家网络，我们取前2个概率最高的专家，那么门控网络会将一个4096维的向量变为一个4维的向量，softmax之后即是0-3这四个专家各自的概率，假如第0个和第3个专家的概率最大，分别为0.3和0.4，则该token会被送到第0个和第3个专家进行处理，二者的输出再加权求和，权重是被选中的专家的概率的分布，比如专家0的权重为：<strong>0.3&#x2F;0.7 &#x3D; 0.43</strong>。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里我们假设定义n_embed为32， num_experts=4, top_k=2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TopkRouter</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_embed, num_experts, top_k</span>):</span><br><span class="line">        <span class="built_in">super</span>(TopkRouter, self).__init__()</span><br><span class="line">        self.top_k = top_k</span><br><span class="line">        self.linear =nn.Linear(n_embed, num_experts)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, mh_output</span>):</span><br><span class="line">        logits = self.linear(mh_output)    <span class="comment"># (2,4,32) ---&gt; (2,4,4)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># #Noise logits</span></span><br><span class="line">        <span class="comment"># noise_logits = self.noise_linear(mh_output)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># #Adding scaled unit gaussian noise to the logits</span></span><br><span class="line">        <span class="comment"># noise = torch.randn_like(logits)*F.softplus(noise_logits)</span></span><br><span class="line">        <span class="comment"># noisy_logits = logits + noise</span></span><br><span class="line">        <span class="comment"># top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取前K大的值和索引，沿列。</span></span><br><span class="line">        top_k_logits, indices = logits.topk(self.top_k, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 创建一个形状和logits相同全&#x27;-inf&#x27;矩阵，即(2,4,4)</span></span><br><span class="line">        zeros = torch.full_like(logits, <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>))</span><br><span class="line">        <span class="comment"># 按照索引和值填充上述zeros矩阵</span></span><br><span class="line">        sparse_logits = zeros.scatter(-<span class="number">1</span>, indices, top_k_logits)</span><br><span class="line">        <span class="comment"># 对其进行softmax，未被填充的位置会为0</span></span><br><span class="line">        router_output = F.softmax(sparse_logits, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> router_output, indices</span><br></pre></td></tr></table></figure>

<p>代码中注释掉的部分是添加噪声的部分，有时候在训练过程中，某一个专家总是会被选择，而其余专家都很悠闲，我们不希望这种情况发生。上面代码注释掉的部分即是通过添加一些随机的噪声来影响输出的概率分布，希望在开始的时候为专家的选择添加一些随机性。</p>
<h2 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h2><p>在通常的混合专家模型 (MoE) 训练中，门控网络往往倾向于主要激活相同的几个专家。这种情况可能会自我加强，因为受欢迎的专家训练得更快，因此它们更容易被选择。这种情况是我们不愿意看到的，因为这样其余的专家得不到充分的训练，没有达到我们想要的”不同的专家处理不同领域的问题”的初衷。同时从硬件角度来讲，不同的专家网络分布在不同的设备，训练不充分导致硬件资源利用率很低。</p>
<p>为了缓解这个问题，引入了一个辅助损失，旨在鼓励给予所有专家相同的重要性。这个损失确保所有专家接收到大致相等数量的训练样本，从而平衡了专家之间的选择。其中一种比较简单的辅助损失函数如下：</p>
<div style="text-align: center;">
  <style>.luexjsginfdj{width: 40%; height: auto;}</style><img src="/2025/04/07/transformer-MLP/04.png" class="luexjsginfdj" alt="辅助损失函数">
</div>

<p>其中几个参数的含义如下：</p>
<ul>
<li>$f_i$: 路由到第i个专家的比例</li>
<li>$s_{i, t}$: 表示第t个token选择第i个专家的概率</li>
<li>$P_i$: 表示第i个专家在这一整个序列中被选择的概率的平均值</li>
<li>$\alpha$是一个系数</li>
</ul>
<p>由于$f_i$是不可微分的，所以最小化上面这个损失函数等价于最小化 $s_{i, t}$，对$s_{i, t}$的调整也会影响到$f_i$， 最终调整分配给每个专家的负载。上述辅助损失和模型的交叉熵损失是添加在一起的。如下式：</p>
<p>$$<br>L_{total} &#x3D; L_{main} + \lambda \cdot L_{balance} \tag{1.3}<br>$$</p>
<p>然而，使用这种辅助损耗来平衡负载是有代价的，因为其梯度可能会干扰语言建模目标的梯度，导致模型性能下降，综上所述，辅助损耗控制的负载均衡是一把双刃剑，如果alpha值未经仔细调整，可能会对模型性能产生负面影响。在实际应用中，由于资源限制，在大型语言模型训练期间调整alpha值具有挑战性，这进一步增加了优化过程的复杂性。</p>
<h1 id="Deepseek中的专家架构"><a href="#Deepseek中的专家架构" class="headerlink" title="Deepseek中的专家架构"></a>Deepseek中的专家架构</h1><p>Deepseek-V3中使用的MoE架构可以说是业界比较前沿的架构了。它继承自deepseek-v2，主要有两个亮点，第一个即是使用了路由专家和共享专家的概念。如下图所示：</p>
<div style="text-align: center;">
  <style>.xxjslkwfcghq{width: 60%; height: auto;}</style><img src="/2025/04/07/transformer-MLP/05.png" class="xxjslkwfcghq" alt="辅助损失函数">
</div>

<p>可以看到，所有token都会进入共享专家，所有token也都会进入路由(Router)，而Router最终会决定这些token各自进入哪一个专家。Deepseek的每一个MoE层会有256个路由专家，包含一个共享专家。其核心计算方法如论文中的公式如下：</p>
<div style="text-align: center;">
  <style>.dneanwkwjobu{width: 40%; height: auto;}</style><img src="/2025/04/07/transformer-MLP/06.png" class="dneanwkwjobu" alt="DeepseekMoE">
</div>

<p>其中$u_t$表示当前序列中的第t个token的语义向量。由第一个公式可知，该token会经过所有的共享专家。$N_s$表示共享专家的个数。同时也会经过router为其选择topK个路由专家。$N_r$为路由专家的数量。最终token原本的向量和上面这两个结果加载一起构成MoE层对于单个token的输出。</p>
<p>最后一个公式$s_{i, t}$表示第t个token对应第i个专家的概率，第三个公式表明，处了被选中的topk个专家，其余专家的概率都为0。第二个公式表示被选中的专家的的结果加权求和时的权重。</p>
<h2 id="无损失负载均衡策略"><a href="#无损失负载均衡策略" class="headerlink" title="无损失负载均衡策略"></a>无损失负载均衡策略</h2><p>Deepseek采用了两种负载均衡方法，第一种是没有辅助损失函数的。即在门控网络的输出经过softmax之后的概率上加一个偏差，如下图所示：</p>
<div style="text-align: center;">
  <style>.xurpelllorod{width: 40%; height: auto;}</style><img src="/2025/04/07/transformer-MLP/07.png" class="xurpelllorod" alt="无损失负载均衡">
</div>

<p>这个偏置的逻辑是，如果某些专家被过度选择，则偏置会变为负数从而降低专家被选择的概率。如果某些专家过于闲置，则偏置为负数，提高其被选择的概率。具体计算方式为：</p>
<ul>
<li>首先计算每个专家分配的token数量的平均值</li>
<li>计算每个专家分配的标记数量，利用平均值减去该数量</li>
<li>偏差由该差值（或误差）的符号乘以一个固定的更新率决定，更新率是一个可调的超参数。<br>由此可见，如果一个专家被过度选择，则这个差值则为负数。如下所示：</li>
</ul>
<div style="text-align: center;">
  <style>.fwmolexiwsbm{width: 60%; height: auto;}</style><img src="/2025/04/07/transformer-MLP/08.png" class="fwmolexiwsbm" alt="偏差的计算方式">
</div>

<p>当然了，Deepseek同样也使用了辅助损失函数，这个函数和上面提到的辅助损失函数形式比较像，我先研究研究再说。</p>
<h2 id="MoE的优点"><a href="#MoE的优点" class="headerlink" title="MoE的优点"></a>MoE的优点</h2><ul>
<li>第一个显而易见的则是我们使用它的初衷，不同的FFN层处理不同的任务，我们希望对于一个token序列，每个专家擅长处理不同方面的语义，不同的专家可以学习到不同的特征，从而使得模型能更精细的处理更复杂的输入。</li>
<li>模型容量得到大大扩展。MoE 允许模型拥有大量的参数（每个专家都有自己的参数），而每个输入只需激活少量专家。通过增加专家数量，可以显著扩展模型的参数量，而不会显著增加计算成本（因为每个输入只激活少数几个专家）。</li>
</ul>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/ghj1976/p/18676819/deepseekv3-de-moe-jia-gou-jie-xi-xi-li-du-zhuan-ji">DeepSeek-V3 的 MoE 架构解析：细粒度专家与高效模型扩展</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/15797610465">[Deepseek v3技术报告学习] 2.MoE结构及MoE的负载均衡</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/25228000281">DeepSeek-V3 解读3：无辅助损耗的负载均衡</a></p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/blog/zh/moe">混合专家模型 (MoE) 详解</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/701777558">从零实现一个MOE（专家混合模型）</a></p>
</article><!-- lincense--><div class="license-wrapper"><p> <span>Author:  </span><a href="http://example.com">武丢丢</a></p><p> <span>Link:  </span><a href="http://example.com/2025/04/07/transformer-MLP/">http://example.com/2025/04/07/transformer-MLP/</a></p><p> <span>Copyright:  </span><span>All articles in this blog are licensed under <a rel="license noopener" target="_blank" href="https://creativecommons.org/licenses/by-nc-nd/3.0">CC BY-NC-SA 3.0</a> unless stating additionally.</span></p></div><div class="post-paginator"><a class="nextSlogan" href="/2025/04/07/RAG-langchain/" title="RAG-langchain"><span>NextPost ></span><br><span class="nextTitle">RAG-langchain</span></a><div class="clear"></div></div><div id="comment"></div></section></article><footer id="cxo-footer-outer"><div id="cxo-footer-inner"><p class="footer-container"><span>Site by </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span> | theme </span><a target="_blank" rel="noopener" href="https://github.com/Longlongyu/hexo-theme-Cxo"><span>Cxo</span></a></p><i class="fa fa-user"> </i><span id="busuanzi_value_site_uv"></span><span> | </span><i class="fa fa-eye"> </i><span id="busuanzi_value_site_pv"></span></div></footer><!-- catelog--><div class="toc-wrapper" style="top: 70vh;"><div class="toc-catalog"><i class="fa fa-list"> </i><span>CATALOG</span></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E6%9C%89MLP%EF%BC%9F"><span class="toc-number">1.</span> <span class="toc-text">为什么会有MLP？</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#MLP%E7%9A%84%E6%9E%B6%E6%9E%84"><span class="toc-number">2.</span> <span class="toc-text">MLP的架构</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#MoE"><span class="toc-number">3.</span> <span class="toc-text">MoE</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#MoE%E7%9A%84%E7%AE%80%E5%8F%B2"><span class="toc-number">3.1.</span> <span class="toc-text">MoE的简史</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MoE%E7%9A%84%E6%9E%84%E6%88%90"><span class="toc-number">3.2.</span> <span class="toc-text">MoE的构成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1"><span class="toc-number">3.3.</span> <span class="toc-text">负载均衡</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Deepseek%E4%B8%AD%E7%9A%84%E4%B8%93%E5%AE%B6%E6%9E%B6%E6%9E%84"><span class="toc-number">4.</span> <span class="toc-text">Deepseek中的专家架构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A0%E6%8D%9F%E5%A4%B1%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E7%AD%96%E7%95%A5"><span class="toc-number">4.1.</span> <span class="toc-text">无损失负载均衡策略</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MoE%E7%9A%84%E4%BC%98%E7%82%B9"><span class="toc-number">4.2.</span> <span class="toc-text">MoE的优点</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">5.</span> <span class="toc-text">参考资料</span></a></li></ol></div><!-- top--><i class="fa fa-arrow-up close" id="go-up" aria-hidden="true"></i></body></html>