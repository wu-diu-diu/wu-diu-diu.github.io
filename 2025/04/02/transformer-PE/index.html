<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><meta name="author" content="武丢丢"><meta name="renderer" content="webkit"><meta name="copyright" content="武丢丢"><meta name="keywords" content="武丢丢"><meta name="description" content="null"><meta name="Cache-Control" content="no-cache"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><title>transformer-位置编码 · Punchy's Blog</title><link rel="stylesheet" href="/css/style.css?v=2018.7.9"><link rel="stylesheet" href="/css/animation.css?v=2018.7.9"><link rel="icon" href="/img/assets/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css?v=1.0.0"><!-- scripts--><script>(function( w ){
  "use strict";
  // rel=preload support test
  if( !w.loadCSS ){
    w.loadCSS = function(){};
  }
  // define on the loadCSS obj
  var rp = loadCSS.relpreload = {};
  // rel=preload feature support test
  // runs once and returns a function for compat purposes
  rp.support = (function(){
    var ret;
    try {
      ret = w.document.createElement( "link" ).relList.supports( "preload" );
    } catch (e) {
      ret = false;
    }
    return function(){
      return ret;
    };
  })();

  // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
  // then change that media back to its intended value on load
  rp.bindMediaToggle = function( link ){
    // remember existing media attr for ultimate state, or default to 'all'
    var finalMedia = link.media || "all";

    function enableStylesheet(){
      link.media = finalMedia;
    }

    // bind load handlers to enable media
    if( link.addEventListener ){
      link.addEventListener( "load", enableStylesheet );
    } else if( link.attachEvent ){
      link.attachEvent( "onload", enableStylesheet );
    }

    // Set rel and non-applicable media type to start an async request
    // note: timeout allows this to happen async to let rendering continue in IE
    setTimeout(function(){
      link.rel = "stylesheet";
      link.media = "only x";
    });
    // also enable media after 3 seconds,
    // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
    setTimeout( enableStylesheet, 3000 );
  };

  // loop through link elements in DOM
  rp.poly = function(){
    // double check this to prevent external calls from running
    if( rp.support() ){
      return;
    }
    var links = w.document.getElementsByTagName( "link" );
    for( var i = 0; i < links.length; i++ ){
      var link = links[ i ];
      // qualify links to those with rel=preload and as=style attrs
      if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
        // prevent rerunning on link
        link.setAttribute( "data-loadcss", true );
        // bind listeners to toggle media back
        rp.bindMediaToggle( link );
      }
    }
  };

  // if unsupported, run the polyfill
  if( !rp.support() ){
    // run once at least
    rp.poly();

    // rerun poly on an interval until onload
    var run = w.setInterval( rp.poly, 500 );
    if( w.addEventListener ){
      w.addEventListener( "load", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    } else if( w.attachEvent ){
      w.attachEvent( "onload", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    }
  }


  // commonjs
  if( typeof exports !== "undefined" ){
    exports.loadCSS = loadCSS;
  }
  else {
    w.loadCSS = loadCSS;
  }
}( typeof global !== "undefined" ? global : this ) );</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" defer></script><script src="/js/main.js?v=2018.7.9" defer></script><!-- fancybox--><link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script><!-- busuanzi--><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><meta name="generator" content="Hexo 7.1.1"></head><body><section class="profile-close" id="cxo-profile"><div class="profile-avatar"><i class="fa fa-caret-left"></i><img src="/img/assets/me.png"></div><!--.profile-saying
  i.fa.fa-comment
  .saying--><div class="cxo-profile-inner"><div class="profile-name">punchy</div><div class="profile-signature">Stay foolish Stay hungry</div><div class="read-progress"></div></div></section><header id="cxo-intro" style="height: 70vh;background-image: url(/img/intro/bg.jpg); background-repeat: no-repeat; background-position: top center; background-size: cover; -webkit-background-size: cover; -o-background-size: cover;"><nav id="cxo-intro-nav"><section><div class="intro-nav-title"><a href="/">Wudiudiu's Blog</a></div><div class="intro-nav-label-box"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div><i class="fa fa-bars intro-nav-menu"><div class="intro-nav-drop"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div></i><div class="clear"></div></section></nav><h1 class="post-title">transformer-位置编码</h1><div class="post-intros"><div class="post-intro-meta"><span class="post-intro-time"><i class="post-intro-calendar fa fa-calendar"></i><span>2025-04-02</span></span><span class="post-intro-tags"><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="LLM"> LLM</a></span></div></div></header><article class="cxo-up" id="cxo-content-outer"><section id="cxo-content-inner"><article class="article-entry" id="post"><script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    processEscapes: true,
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

<h1 id="Transformer之位置编码"><a href="#Transformer之位置编码" class="headerlink" title="Transformer之位置编码"></a>Transformer之位置编码</h1><h2 id="为什么需要位置编码"><a href="#为什么需要位置编码" class="headerlink" title="为什么需要位置编码"></a>为什么需要位置编码</h2><p>注意力计算公式如下图所示：  </p>
<div style="text-align: center;">
  <style>.bsivdyaugbfj{width: 40%; height: 40%;}</style><img src="/2025/04/02/transformer-PE/01.png" class="bsivdyaugbfj" alt="注意力计算公式">
</div>

<p>其中q和k分别为query和key，代表token的语义信息。如果没有位置编码，那么q和k的计算结果是不受二者之间相对距离大小的影响的。也就是说，如果没有位置编码，那么transformer就是一个类词袋模型。</p>
<h3 id="词袋模型"><a href="#词袋模型" class="headerlink" title="词袋模型"></a>词袋模型</h3><p> <em>词袋模型：BOW，是一种对文本进行向量化的模型</em><br>词袋模型首先会对文本进行分词，为所有出现过的词给定一个序号，进而得到一个语料库，比如语料库大小为1000。则对于一个句子比如“我爱你”进行语义建模，由于这三个字在语料库中的序号为333，335，874.则这句话的语义向量为：<code>[0,0,0,0,0,...1,0,0,1,0,...,1,0,0,0,0]</code>即在对应索引处置1即可。这样带来一个问题即’你爱我’和’我爱你’这两句意思不同的话的语义向量是相同的。所以，<strong>词袋模型不考虑词语顺序，只是将词一股脑儿的放进袋子里，根据句子内词语出现的次数来进行语义建模</strong>。</p>
<h3 id="Attention机制的缺陷"><a href="#Attention机制的缺陷" class="headerlink" title="Attention机制的缺陷"></a>Attention机制的缺陷</h3><p>attention机制是transformer的核心，其思想是计算每个token与其余token的相似度，利用这个相似度去更新token的语义。从上面的计算公式可以看出，这种相似度计算是全局的，是位置无关的，无论两个token是相近还是相远，注意力分数都是一样的。下面这个代码将这个特性展现了出来：</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">d = <span class="number">8</span> <span class="comment"># 词嵌入维度</span></span><br><span class="line">t = <span class="number">3</span> <span class="comment"># 句子长度</span></span><br><span class="line">q = torch.randn(<span class="number">1</span>,d) <span class="comment"># 我</span></span><br><span class="line">k = torch.randn(t,d) <span class="comment"># 我爱你</span></span><br><span class="line">v = torch.randn(t,d) <span class="comment"># 我爱你</span></span><br><span class="line"></span><br><span class="line">w = q@k.transpose(<span class="number">1</span>,<span class="number">0</span>)</span><br><span class="line">w1 = F.softmax(w,dim=<span class="number">1</span>)</span><br><span class="line">result = w1 @ v</span><br><span class="line"></span><br><span class="line">k_shift = k[[<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>],:] <span class="comment"># 你爱我</span></span><br><span class="line">v_shift = v[[<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>],:] <span class="comment"># 你爱我</span></span><br><span class="line">shift_w = q@k_shift.transpose(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">shift_w1 = F.softmax(shift_w, dim=<span class="number">1</span>)</span><br><span class="line">shift_result = shift_w1 @ v_shift</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(result, shift_result))</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>

<p>即’我’在 我爱你 和 你爱我 中的语义是一样的，这是肯定不对的。<strong>所以，就需要我们在transformer中添加位置编码，让模型在计算token之间的相似度时，能知晓两个token之间的相对距离</strong></p>
<h2 id="有几种位置编码"><a href="#有几种位置编码" class="headerlink" title="有几种位置编码"></a>有几种位置编码</h2><p>位置编码主要分为两种，第一种是想办法将位置信息融入到输入中，这是绝对位置编码的一般做法。另一种是修改一下Attention机制的结构，使其能够在计算注意力分数时，考虑到位置信息，这构成了相对位置编码的一般做法。我们首先介绍一下绝对位置编码。</p>
<h3 id="绝对位置编码"><a href="#绝对位置编码" class="headerlink" title="绝对位置编码"></a>绝对位置编码</h3><p>其公式如下：  </p>
<div style="text-align: center;">
  <style>.jwdilkeihbee{width: 40%; height: 40%;}</style><img src="/2025/04/02/transformer-PE/position_encoding.png" class="jwdilkeihbee" alt="位置编码">
</div>

<p>其中pos代表单个token的位置索引，比如1000个token，这个pos的取值则为0-999。i表示同一个token的向量的某一个维度索引，其中偶数维都有sin函数计算，奇数维都由cos函数计算。其代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)  <span class="comment">## (max_len, 1)</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) * (-math.log(<span class="number">10000.0</span>) / d_model))  <span class="comment">## d_model/2 中结果</span></span><br><span class="line"></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)  <span class="comment">## 在第一个维度添加一个维度，便于批处理  (1, max_len, d_model)</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + torch.tensor(self.pe[:, :x.size(<span class="number">1</span>)], requires_grad=<span class="literal">False</span>)  <span class="comment">## max_len是模型能接受的最大长度token输入，推理时token长度是不定的，x.size()是B,T,C</span></span><br><span class="line">        <span class="comment">## x.size(1)是输入的token长度，则将输入token长度对应的位置编码加入到计算中</span></span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>

<p>为什么要使用这种sin和cos交替的形式来表示一个token的位置信息呢？transformer作者在论文中并没有明说。但是根据公式的形式，我们可以发现这个位置编码的几个性质：  </p>
<ul>
<li>有界性：sin函数和cos函数都是有界的，所以位置编码的值不会太大也不会太小，在-1到1的范围之内</li>
<li>周期性：不同token的同一个维度的位置编码值可以看作是同一个频率的正余弦向量，因为分母是一样大的，只是分子不同。即一个1000维的位置向量，其低维处的变量是高频频变化的，高维处的变量是低频变化的。周期性可以使得位置编码在处理较长的序列时，仍然能生成合理的值。</li>
<li>叠加性：对于同一个token而言，每两个不同的维度对，比如0-1和2-3，都是不同频率的正余弦向量在同一个点处的值。任意周期函数都可以用傅里叶展开公式变为三角函数的无穷级数。这里可以用这种方式理解，即叠加不同频率的正余弦函数，试图表示token的位置信息</li>
<li>能够反映一定的相对位置：PE(x+k)可以用PE(x)来线性表示。因为sin(x+k)&#x3D;sin(x)cosk + cos(x)sink，其中sink和cosk看作常数。即给定距离k和当前位置，k距离处的位置编码是当前位置关于距离k的线性组合</li>
<li>远程衰减：两个位置编码的点积取决于二者之间的相对位置，即两个位置编码的点积值可以反映其相对位置的大小。由下图可知，当相对位置增大时，点积的值是在减小的。而且这种点积具有对称性。</li>
<li>综上所述，绝对位置编码能够反映token之间的绝对位置信息，同时也能够一定程度上反映token之间的相对位置。</li>
</ul>
<div style="display: flex; justify-content: space-between;">
  <style>.vnygkuncbtud{width: 70%; height: auto;}</style><img src="/2025/04/02/transformer-PE/02.png" class="vnygkuncbtud" alt="相距为k的向量的点积">
  <style>.fedrlftefgiv{width: 70%; height: auto;}</style><img src="/2025/04/02/transformer-PE/03.png" class="fedrlftefgiv" alt="远程衰减">
</div>

<p>所以，综上所述，绝对位置编码能够较好的表示token的位置信息，同时，也能够表示一定的相对位置信息。但是<strong>在实际使用中，绝对位置编码的外推性和远程衰减的特性都不能很好的展现</strong>。原因如下：</p>
<div style="text-align: center;">
  <style>.bqohwiicbsxm{width: 60%; height: auto;}</style><img src="/2025/04/02/transformer-PE/04.png" class="bqohwiicbsxm" alt="图4:经过Attention层的位置编码">
</div>

<p>由上图可知，位置编码是和word_embedding加和在一起的，经过注意力层的Q和K点积之后，其远程衰减的特性消失了，那么其表达相对位置的能力就不存在了。Q和K点积的公式如下：<br><span style="font-size: 80%;"><br>$$<br>\begin{align*}<br>q_{t}^{T} * k_{t+\Delta t} &#x3D; \left[W_{Q} * (x_{t} + PE_{t})\right]^{T} \left[W_{K} * (x_{t+\Delta t} + PE_{t+\Delta t})\right] &#x3D; \left[x_{t}^{T} W_{Q}^{T} + PE_{t}^{T} W_{Q}^{T}\right] \left[W_{K} x_{t+\Delta t} + W_{K} PE_{t+\Delta t}\right]<br>\end{align*} \tag{1.1}<br>$$<br><span></p>
<p>由公式可知，q与一个相距为$\Delta t$的k相乘的结果是$x_{t}^TW_{Q}^TW_{K}x_{t + \Delta t}$，这一部分是注意力分数，后面的部分都是加性的绝对位置编码的带来的部分，由图4可知，这种冗余部分破坏了位置编码的相对位置属性，所以这就是绝对位置编码在注意力计算中无法表达相对位置信息的原因，也是加性的位置编码的弊端。</p>
<h3 id="旋转位置编码"><a href="#旋转位置编码" class="headerlink" title="旋转位置编码"></a>旋转位置编码</h3><p>为了解决加性位置编码的弊端，之后便出现了乘性的位置编码。旋转位置编码的思想是使用相乘的方法令每一个token都含有自己的绝对位置信息，那么在两个token相乘的时候，结果中自动的就会包含了相对位置信息。</p>
<p>首先我们需要回顾一下线性代数中旋转矩阵的知识。维基百科定义为：<strong>旋转矩阵是在乘以一个向量的时候，改变了向量的方向但不改变向量大小的矩阵</strong>。在二维空间中，旋转矩阵表示如下：</p>
<p>$$<br>M(\theta) &#x3D; \begin{bmatrix}<br>\cos\theta &amp; -\sin\theta \\<br>\sin\theta &amp; \cos\theta<br>\end{bmatrix} \tag{1.2}<br>$$</p>
<p>$M(\theta)$就是一个旋转矩阵，由三角函数的和角公式，我们可以得到两个旋转矩阵相乘的结果为：<br><span style="font-size: 80%;"><br>$$<br>M(\theta)^T \cdot M(\alpha) &#x3D; \begin{bmatrix}<br>\cos\theta &amp; \sin\theta  \\<br>-\sin\theta &amp; \cos\theta<br>\end{bmatrix} \cdot \begin{bmatrix}<br>\cos\alpha &amp; -\sin\alpha \\<br>\sin\alpha &amp; \cos\alpha<br>\end{bmatrix} &#x3D; \begin{bmatrix}<br>\cos(\alpha - \theta) &amp; -\sin(\alpha - \theta) \\<br>\sin(\alpha - \theta) &amp; \cos(\alpha - \theta)<br>\end{bmatrix} &#x3D; M(\alpha - \theta) \tag{1.3}<br>$$<br></span></p>
<p>由上式可知，两个带有旋转角度信息的旋转矩阵进行向量乘法，其结果就带有了相对信息。那么如果我们对每一个token都旋转一个角度，这个角度与其绝对位置有关，之后进行qk相乘的时候，注意力分数中就会带有相对位置信息。公式如下：<br>$$<br>(R_m \cdot x)^T * (R_n \cdot y) &#x3D; x^T \cdot R_m^T \cdot R_n \cdot y &#x3D; x^T \cdot R_{n-m} \cdot y \tag{1.4}<br>$$</p>
<p>那么既然相对位置信息是在query和key相乘的时候自动添加的，那么关键就在于如何为每个token的query和key向量添加绝对位置信息。其实很简单，将每个query和key向量，旋转位置索引个单位角度$\theta$即可。则注意力公式变为：<br>$$<br>Attention(q_m, k_n) &#x3D; (R_m \cdot q_m)^T \cdot (R_n \cdot k_n) \tag{1.5}<br>$$<br>其中$R_m$等于如下：<br>$$<br>R_m &#x3D; \begin{bmatrix}<br>\cos m\theta &amp; -\sin m\theta \\<br>\sin m\theta &amp; \cos m\theta<br>\end{bmatrix} \tag{1.6}<br>$$<br>m为qk的位置索引，即pos，$\theta$仍然沿用正余弦绝对位置编码的设置即：<br>$$<br>\theta_i &#x3D; 10000^{-2i&#x2F;d} \tag{1.7}<br>$$<br>如果query和key不是二维呢？在实际的transformer中，q和k都是高维向量，那么对于高维的向量，我们将其维度两两分为一组，比如1000维的query向量，$q_0$和$q_1$可以看作一组，令其与式(1.6)相乘，则将这一组向量赋予了绝对位置信息。公式如下：<br>$$<br>\mathbf{R_m \cdot q_m} &#x3D; \begin{bmatrix}<br>\cos m\theta_0 &amp; -\sin m\theta_0 \\<br>\sin m\theta_0 &amp; \cos m\theta_0<br>\end{bmatrix} \cdot \begin{bmatrix}<br>\ q_0 \\<br>\ q_1<br>\end{bmatrix} \tag{1.8}<br>$$<br>所以，对于一整个高维的query向量来说，都两两乘以一个旋转矩阵，如图下：</p>
<div style="text-align: center;">
  <style>.vjnxcuqrpsqh{width: 60%; height: auto;}</style><img src="/2025/04/02/transformer-PE/06.png" class="vjnxcuqrpsqh" alt="图6:高维query向量的旋转">
</div>

<p>上式相当于使用一个高维的，稀疏的，对角矩阵乘以query向量，当然我们不会创建整个稀疏矩阵，太大太占用内存了。所以对于一个1000维的query而言，我们可以首先计算上图中的cos和sin的部分，假如整个token序列的最大长度为500，m取值为0-499，$\theta$的指数部分取值为：<code>(0, 2, 4, .....,994, 996, 998) / 1000</code>，得到上图中的cos_table和sin_table, 都为一个$500 \cdot 1000$的矩阵。对于每一个$q_m$,乘以cos_table和sin_table中对应位置的$1 \cdot 1000$的向量即可。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_sin_cos_cache</span>(<span class="params">max_num_tokens, head_size</span>):</span><br><span class="line">    theta = <span class="number">10000</span> ** (-np.arange(<span class="number">0</span>, head_size, <span class="number">2</span>) / head_size)</span><br><span class="line">    theta = theta.reshape(-<span class="number">1</span>, <span class="number">1</span>).repeat(<span class="number">2</span>, axis=<span class="number">1</span>).flatten()</span><br><span class="line"></span><br><span class="line">    pos = np.arange(<span class="number">0</span>, max_num_tokens)</span><br><span class="line">    table = pos.reshape(-<span class="number">1</span>, <span class="number">1</span>) @ theta.reshape(<span class="number">1</span>, -<span class="number">1</span>)  <span class="comment"># [max_num_tokens, head_size]</span></span><br><span class="line"></span><br><span class="line">    sin_cache = np.sin(table)</span><br><span class="line">    sin_cache[:, ::<span class="number">2</span>] = -sin_cache[:, ::<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    cos_cache = np.cos(table)</span><br><span class="line">    <span class="keyword">return</span> sin_cache, cos_cache</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rotate_half</span>(<span class="params">vec</span>):</span><br><span class="line">    <span class="keyword">return</span> vec.reshape(-<span class="number">1</span>, <span class="number">2</span>)[:, ::-<span class="number">1</span>].flatten()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rotary</span>(<span class="params">vec, pos, sin_table, cos_table</span>):</span><br><span class="line">    <span class="keyword">return</span> vec * cos_table[pos] + rotate_half(vec) * sin_table[pos]</span><br></pre></td></tr></table></figure>

<p>其实，通过旋转的方式注入绝对位置信息这种办法，在正余弦位置编码中也可以体现。假设$c_i &#x3D; 1 &#x2F; 10000^{-2i&#x2F;d}$,那么第t个位置的绝对位置编码可以表示为：<br>$$<br>PE_t &#x3D; \begin{bmatrix}<br>\sin t \cdot c_i \\<br>\cos t\cdot c_i<br>\end{bmatrix} \tag{1.9}<br>$$</p>
<p>第$t + k$位置的绝对位置编码$PE_{t+k}$可以表示为：<br>$$<br>\begin{aligned}<br>PE_{t+k} &amp;&#x3D;<br>\begin{bmatrix}<br>\sin (t + k) \cdot c_i \\<br>\cos (t + k) \cdot c_i<br>\end{bmatrix}<br>\\<br>&amp;&#x3D;<br>\begin{bmatrix}<br>\sin (t \cdot c_i) \cdot \cos (k \cdot c_i) + \cos (t \cdot c_i) \cdot \sin (k \cdot c_i) \\<br>\cos (t \cdot c_i) \cdot \cos (k \cdot c_i) - \sin (t \cdot c_i) \cdot \sin (k \cdot c_i)<br>\end{bmatrix}<br>\\<br>&amp;&#x3D;<br>\begin{bmatrix}<br>\cos (k \cdot c_i) &amp; \sin (k \cdot c_i) \\<br>-\sin (k \cdot c_i) &amp; \cos (k \cdot c_i)<br>\end{bmatrix} \cdot \begin{bmatrix}<br>\sin (t \cdot c_i) \\<br>\cos (t\cdot c_i)<br>\end{bmatrix}<br>\\<br>&amp;&#x3D;<br>\begin{bmatrix}<br>\cos (k \cdot c_i) &amp; \sin (k \cdot c_i) \\<br>-\sin (k \cdot c_i) &amp; \cos (k \cdot c_i)<br>\end{bmatrix} \cdot PE_t<br>\end{aligned}<br>$$</p>
<p>将$c_i &#x3D; 1 &#x2F; 10000^{-2i&#x2F;d}$视作一个常量，则$PE_t$和$PE_{t+k}$的关系为$PE_{t+k} &#x3D; R_k^T \cdot PE_t$其中:<br>$$<br>R_k^T &#x3D; \begin{bmatrix}<br>\cos (k \cdot c_i) &amp; -\sin (k \cdot c_i) \\<br>\sin (k \cdot c_i) &amp; \cos (k \cdot c_i)<br>\end{bmatrix}<br>$$</p>
<p>将上述公式展开如图下：</p>
<div style="text-align: center;">
  <style>.ephpyntploeu{width: 60%; height: auto;}</style><img src="/2025/04/02/transformer-PE/07.png" class="ephpyntploeu" alt="图7:PEt+k由PEt旋转得到">
</div>

<p>是不是和旋转位置编码很像？没错，绝对位置编码可以看作是使用旋转的方式将绝对位置信息注入位置编码中，但是，使用加和的方式破坏了位置信息的远程衰减特性，从而使得模型不能很好的识别相对位置信息。而旋转位置编码使用也是用旋转的方式将绝对位置信息注入到位置编码中，但其是在得到query和key之后注入的，这样后续的注意力计算可以直接利用旋转矩阵的性质而得到相对位置信息。</p>
<p>综上所述，旋转位置编码延续了绝对位置编码的一些特性，比如cos和sin的形式，只不过正余弦位置编码的三角函数中是$pos &#x2F; (10000^{-2i&#x2F;d})$而旋转位置编码中是$pos \cdot 10000^{-2i&#x2F;d}$。正是因为如此，所以旋转位置编码也继承了远程衰减的特性。</p>
<h3 id="二维旋转位置编码"><a href="#二维旋转位置编码" class="headerlink" title="二维旋转位置编码"></a>二维旋转位置编码</h3><p>对于一个句子里的token来说，一个数就可以表示其位置。那对于图片呢？图片中某个像素点的位置至少需要两个数才可确定，这是一维和二维的区别。其实和一维是一样的，其旋转矩阵公式如下：</p>
<p>$$<br>R_{x,y} &#x3D;<br>\begin{bmatrix}<br>\cos x\theta &amp; -\sin x\theta &amp; 0 &amp; 0 \\<br>\sin x\theta &amp; \cos x\theta &amp; 0 &amp; 0 \\<br>0 &amp; 0 &amp; \cos y\theta &amp; -\sin y\theta \\<br>0 &amp; 0 &amp; \sin y\theta &amp; \cos y\theta<br>\end{bmatrix} \tag{1.10}<br>$$</p>
<p>可见，二维RoPE旋转矩阵是一维RoPE的分块矩阵。其和query向量的相乘方法和图7是一样的，所以实际上它相当于将输入向量分成了两半，一半施加x的一维RoPE，一般施加y的RoPE。由此我们不难推出三维和四维的做法。</p>
<h2 id="源码中的RoPE"><a href="#源码中的RoPE" class="headerlink" title="源码中的RoPE"></a>源码中的RoPE</h2><p>这里以chatglm的模型为例，模型设置如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">config = GlmConfig(</span><br><span class="line">    hidden_size=<span class="number">4096</span>//<span class="number">2</span>, </span><br><span class="line">    intermediate_size=<span class="number">13696</span>, </span><br><span class="line">    num_hidden_layers=<span class="number">40</span>//<span class="number">2</span>, </span><br><span class="line">    num_attention_heads=<span class="number">32</span>//<span class="number">2</span>, </span><br><span class="line">    max_position_embeddings=<span class="number">131072</span>)</span><br></pre></td></tr></table></figure>

<p>将batch设置为1，token数设置为14，则hidden_states会输入到rotary_emb函数，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## hidden_states : (1,14,2048) position_ids为一维数组，从0-13</span></span><br><span class="line">position_embeddings = self.rotary_emb(hidden_states, position_ids)</span><br></pre></td></tr></table></figure>

<p>这行代码会计算出图6中的cos和sin向量，根据绝对位置索引。其中position_embeddings是一个元组，包含了cos和sin，二者的大小都为<code>(1,14,64)</code>为什么是64呢，由上面的模型配置可知，注意力头数是16，隐藏层维度是2048，则每个头的维度为2048&#x2F;16&#x3D;128。即query向量的shape是<code>(1,16,14,128)</code>。但这也对不上啊，按照图6，cos和query向量的维度应该是一样的。别急，看下面的代码就知道了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">rotate_half</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Rotates half the hidden dims of the input.&quot;&quot;&quot;</span></span><br><span class="line">    x1 = x[..., <span class="number">0</span>::<span class="number">2</span>]  <span class="comment">## 取出向量中的偶数索引</span></span><br><span class="line">    x2 = x[..., <span class="number">1</span>::<span class="number">2</span>]  <span class="comment">## 取出向量中的奇数索引 </span></span><br><span class="line">    <span class="comment">## 将奇数索引加一个负号和偶数索引竖着堆好再展平，其实就是将图6第二部分sin的符号转移到query上面。</span></span><br><span class="line">    <span class="keyword">return</span> torch.stack((-x2, x1), dim=-<span class="number">1</span>).flatten(-<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply_rotary_pos_emb</span>(<span class="params">q, k, cos, sin, position_ids=<span class="literal">None</span>, unsqueeze_dim=<span class="number">1</span></span>):</span><br><span class="line">    <span class="comment">## Applies Rotary Position Embedding to the query and key tensors.</span></span><br><span class="line"></span><br><span class="line">    cos = cos.unsqueeze(unsqueeze_dim)  <span class="comment">## (1,14,64) -&gt; (1,1,14,64)</span></span><br><span class="line">    sin = sin.unsqueeze(unsqueeze_dim)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Interleave them instead of usual shape</span></span><br><span class="line">    <span class="comment">## 操作cos和sin的最后一维，将最后一维截取一半并重复两次，</span></span><br><span class="line">    <span class="comment">## 如果原始 cos 的最后一维是 [a, b, c, d]，经过上述操作后会变成 [a, a, b, b, c, c, d, d]。</span></span><br><span class="line">    <span class="comment">## 大小仍为： (1,1,14,64)</span></span><br><span class="line">    cos = cos[..., : cos.shape[-<span class="number">1</span>] // <span class="number">2</span>].repeat_interleave(<span class="number">2</span>, dim=-<span class="number">1</span>)</span><br><span class="line">    sin = sin[..., : sin.shape[-<span class="number">1</span>] // <span class="number">2</span>].repeat_interleave(<span class="number">2</span>, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Keep half or full tensor for later concatenation</span></span><br><span class="line">    rotary_dim = cos.shape[-<span class="number">1</span>]</span><br><span class="line">    <span class="comment">## q_rot : (1,16,14,64) 即每个头只有一半的维度需要加入位置编码。</span></span><br><span class="line">    q_rot, q_pass = q[..., :rotary_dim], q[..., rotary_dim:]</span><br><span class="line">    k_rot, k_pass = k[..., :rotary_dim], k[..., rotary_dim:]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Apply rotary embeddings on the first half or full tensor</span></span><br><span class="line">    <span class="comment">## 注入绝对位置信息</span></span><br><span class="line">    q_embed = (q_rot * cos) + (rotate_half(q_rot) * sin)</span><br><span class="line">    k_embed = (k_rot * cos) + (rotate_half(k_rot) * sin)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Concatenate back to full shape</span></span><br><span class="line">    <span class="comment">## 将注入位置信息的部分和没有注入的部分合并</span></span><br><span class="line">    q_embed = torch.cat([q_embed, q_pass], dim=-<span class="number">1</span>)</span><br><span class="line">    k_embed = torch.cat([k_embed, k_pass], dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> q_embed, k_embed</span><br></pre></td></tr></table></figure>

<p>由代码q_rot和q_pass可知，和上面介绍的query的所有维度都做旋转变换不同，chatglm中只使用了前一半的维度来做旋转位置编码。为什么这么做呢？</p>
<ul>
<li>旋转位置编码的核心思想是通过旋转操作嵌入位置信息，但并非所有维度都需要位置信息。</li>
<li>对于一些维度（如语义特征），位置信息可能无关紧要，因此可以直接跳过旋转操作。</li>
<li>这种分块处理方式既减少了计算开销，又保留了模型的灵活性。</li>
</ul>
<p>这里值得注意的是key的shape:<code>1,2,14,128</code>，可见key向量只有2个头，而query有16个头。这和我们通常理解的MHA(Multi-head Attention)不同。是的，chatglm使用的是更流行的<strong>GQA：Group Query Attention</strong>，即多个query公用一个key，这里是8个query共用一个key。到时候key会通过复制操作变为<code>1,16,14,128</code>，但前八个是一样的，后八个也是一样的。后续会写一篇博客介绍注意力机制及其变体。</p>
<p>综上，我们总结了位置编码存在的意义以及几种常见的位置编码，目前旋转位置编码RoPE已经成为大模型的主流和标配，这也展现了其良好的性能。下次总结一下Normalization。我是punchy，下期再见。</p>
<h2 id="参考连接"><a href="#参考连接" class="headerlink" title="参考连接"></a>参考连接</h2><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/collection/699993636">旋转式位置编码 (RoPE) 知识总结</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/rossiXYZ/p/18744797#26-%E4%B8%89%E8%A7%92%E5%87%BD%E6%95%B0%E7%BC%96%E7%A0%81">探秘Transformer之（8）— 位置编码</a></p>
</article><!-- lincense--><div class="license-wrapper"><p> <span>Author:  </span><a href="http://example.com">武丢丢</a></p><p> <span>Link:  </span><a href="http://example.com/2025/04/02/transformer-PE/">http://example.com/2025/04/02/transformer-PE/</a></p><p> <span>Copyright:  </span><span>All articles in this blog are licensed under <a rel="license noopener" target="_blank" href="https://creativecommons.org/licenses/by-nc-nd/3.0">CC BY-NC-SA 3.0</a> unless stating additionally.</span></p></div><div class="post-paginator"><a class="prevSlogan" href="/2025/04/04/transformer-Normalization/" title="transformer-Normalization"><span>< PreviousPost</span><br><span class="prevTitle">transformer-Normalization</span></a><a class="nextSlogan" href="/2025/03/31/Linux%E5%AE%89%E8%A3%85comsol/" title="Linux安装comsol"><span>NextPost ></span><br><span class="nextTitle">Linux安装comsol</span></a><div class="clear"></div></div><div id="comment"></div></section></article><footer id="cxo-footer-outer"><div id="cxo-footer-inner"><p class="footer-container"><span>Site by </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span> | theme </span><a target="_blank" rel="noopener" href="https://github.com/Longlongyu/hexo-theme-Cxo"><span>Cxo</span></a></p><i class="fa fa-user"> </i><span id="busuanzi_value_site_uv"></span><span> | </span><i class="fa fa-eye"> </i><span id="busuanzi_value_site_pv"></span></div></footer><!-- catelog--><div class="toc-wrapper" style="top: 70vh;"><div class="toc-catalog"><i class="fa fa-list"> </i><span>CATALOG</span></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer%E4%B9%8B%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">1.</span> <span class="toc-text">Transformer之位置编码</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">1.1.</span> <span class="toc-text">为什么需要位置编码</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.1.</span> <span class="toc-text">词袋模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Attention%E6%9C%BA%E5%88%B6%E7%9A%84%E7%BC%BA%E9%99%B7"><span class="toc-number">1.1.2.</span> <span class="toc-text">Attention机制的缺陷</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%89%E5%87%A0%E7%A7%8D%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">1.2.</span> <span class="toc-text">有几种位置编码</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%9D%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">1.2.1.</span> <span class="toc-text">绝对位置编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%8B%E8%BD%AC%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">1.2.2.</span> <span class="toc-text">旋转位置编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E7%BB%B4%E6%97%8B%E8%BD%AC%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">1.2.3.</span> <span class="toc-text">二维旋转位置编码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BA%90%E7%A0%81%E4%B8%AD%E7%9A%84RoPE"><span class="toc-number">1.3.</span> <span class="toc-text">源码中的RoPE</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%BF%9E%E6%8E%A5"><span class="toc-number">1.4.</span> <span class="toc-text">参考连接</span></a></li></ol></li></ol></div><!-- top--><i class="fa fa-arrow-up close" id="go-up" aria-hidden="true"></i></body></html>