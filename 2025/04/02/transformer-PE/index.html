<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><meta name="author" content="武丢丢"><meta name="renderer" content="webkit"><meta name="copyright" content="武丢丢"><meta name="keywords" content="武丢丢"><meta name="description" content="null"><meta name="Cache-Control" content="no-cache"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><title>transformer-PE · Punchy's Blog</title><link rel="stylesheet" href="/css/style.css?v=2018.7.9"><link rel="stylesheet" href="/css/animation.css?v=2018.7.9"><link rel="icon" href="/img/assets/favicon.ico"><link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css?version=1.5.6"><!-- scripts--><script>(function( w ){
  "use strict";
  // rel=preload support test
  if( !w.loadCSS ){
    w.loadCSS = function(){};
  }
  // define on the loadCSS obj
  var rp = loadCSS.relpreload = {};
  // rel=preload feature support test
  // runs once and returns a function for compat purposes
  rp.support = (function(){
    var ret;
    try {
      ret = w.document.createElement( "link" ).relList.supports( "preload" );
    } catch (e) {
      ret = false;
    }
    return function(){
      return ret;
    };
  })();

  // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
  // then change that media back to its intended value on load
  rp.bindMediaToggle = function( link ){
    // remember existing media attr for ultimate state, or default to 'all'
    var finalMedia = link.media || "all";

    function enableStylesheet(){
      link.media = finalMedia;
    }

    // bind load handlers to enable media
    if( link.addEventListener ){
      link.addEventListener( "load", enableStylesheet );
    } else if( link.attachEvent ){
      link.attachEvent( "onload", enableStylesheet );
    }

    // Set rel and non-applicable media type to start an async request
    // note: timeout allows this to happen async to let rendering continue in IE
    setTimeout(function(){
      link.rel = "stylesheet";
      link.media = "only x";
    });
    // also enable media after 3 seconds,
    // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
    setTimeout( enableStylesheet, 3000 );
  };

  // loop through link elements in DOM
  rp.poly = function(){
    // double check this to prevent external calls from running
    if( rp.support() ){
      return;
    }
    var links = w.document.getElementsByTagName( "link" );
    for( var i = 0; i < links.length; i++ ){
      var link = links[ i ];
      // qualify links to those with rel=preload and as=style attrs
      if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
        // prevent rerunning on link
        link.setAttribute( "data-loadcss", true );
        // bind listeners to toggle media back
        rp.bindMediaToggle( link );
      }
    }
  };

  // if unsupported, run the polyfill
  if( !rp.support() ){
    // run once at least
    rp.poly();

    // rerun poly on an interval until onload
    var run = w.setInterval( rp.poly, 500 );
    if( w.addEventListener ){
      w.addEventListener( "load", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    } else if( w.attachEvent ){
      w.attachEvent( "onload", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    }
  }


  // commonjs
  if( typeof exports !== "undefined" ){
    exports.loadCSS = loadCSS;
  }
  else {
    w.loadCSS = loadCSS;
  }
}( typeof global !== "undefined" ? global : this ) );</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" defer></script><script src="/js/main.js?v=2018.7.9" defer></script><!-- fancybox--><link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script><!-- busuanzi--><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><meta name="generator" content="Hexo 7.1.1"></head><body><section class="profile-close" id="cxo-profile"><div class="profile-avatar"><i class="fa fa-caret-left"></i><img src="/img/assets/me.png"></div><!--.profile-saying
  i.fa.fa-comment
  .saying--><div class="cxo-profile-inner"><div class="profile-name">punchy</div><div class="profile-signature">细想全是问题，去做才有答案</div><div class="friends"><div>FRIENDS</div><span><a href="//github.com/Longlongyu" target="_black">friendA</a></span><span><a href="//github.com/" target="_black">friendB</a></span><span><a href="//github.com/" target="_black">friendC</a></span></div><div class="read-progress"></div></div></section><header id="cxo-intro" style="height: 70vh;background-image: url(/img/intro/bg.jpg);"><nav id="cxo-intro-nav"><section><div class="intro-nav-title"><a href="/">Wudiudiu's Blog</a></div><div class="intro-nav-label-box"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div><i class="fa fa-bars intro-nav-menu"><div class="intro-nav-drop"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div></i><div class="clear"></div></section></nav><h1 class="post-title">transformer-PE</h1><div class="post-intros"><div class="post-intro-meta"><span class="post-intro-time"><i class="post-intro-calendar fa fa-calendar"></i><span>2025-04-02</span></span><span class="post-intro-tags"><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="LLM"> LLM</a></span></div></div></header><article class="cxo-up" id="cxo-content-outer"><section id="cxo-content-inner"><article class="article-entry" id="post"><h1 id="Transformer之位置编码"><a href="#Transformer之位置编码" class="headerlink" title="Transformer之位置编码"></a>Transformer之位置编码</h1><h2 id="为什么需要位置编码"><a href="#为什么需要位置编码" class="headerlink" title="为什么需要位置编码"></a>为什么需要位置编码</h2><p>注意力计算公式如下图所示：  </p>
<div style="text-align: center;">

<p><img src="/01.png" alt="attention"></p>
</div>

<p>其中q和k分别为query和key，代表token的语义信息。如果没有位置编码，那么q和k的计算结果是不受二者之间相对距离大小的影响的。也就是说，如果没有位置编码，那么transformer就是一个类词袋模型。</p>
<h3 id="词袋模型"><a href="#词袋模型" class="headerlink" title="词袋模型"></a>词袋模型</h3><p> <em>词袋模型：BOW，是一种对文本进行向量化的模型</em><br>词袋模型首先会对文本进行分词，为所有出现过的词给定一个序号，进而得到一个语料库，比如语料库大小为1000。则对于一个句子比如“我爱你”进行语义建模，由于这三个字在语料库中的序号为333，335，874.则这句话的语义向量为：<code>[0,0,0,0,0,...1,0,0,1,0,...,1,0,0,0,0]</code>即在对应索引处置1即可。这样带来一个问题即’你爱我’和’我爱你’这两句意思不同的话的语义向量是相同的。所以，<strong>词袋模型不考虑词语顺序，只是将词一股脑儿的放进袋子里，根据句子内词语出现的次数来进行语义建模</strong>。</p>
<h3 id="Attention机制的缺陷"><a href="#Attention机制的缺陷" class="headerlink" title="Attention机制的缺陷"></a>Attention机制的缺陷</h3><p>attention机制是transformer的核心，其思想是计算每个token与其余token的相似度，利用这个相似度去更新token的语义。从上面的计算公式可以看出，这种相似度计算是全局的，是位置无关的，无论两个token是相近还是相远，注意力分数都是一样的。下面这个代码将这个特性展现了出来：</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">d = <span class="number">8</span> <span class="comment"># 词嵌入维度</span></span><br><span class="line">t = <span class="number">3</span> <span class="comment"># 句子长度</span></span><br><span class="line">q = torch.randn(<span class="number">1</span>,d) <span class="comment"># 我</span></span><br><span class="line">k = torch.randn(t,d) <span class="comment"># 我爱你</span></span><br><span class="line">v = torch.randn(t,d) <span class="comment"># 我爱你</span></span><br><span class="line"></span><br><span class="line">w = q@k.transpose(<span class="number">1</span>,<span class="number">0</span>)</span><br><span class="line">w1 = F.softmax(w,dim=<span class="number">1</span>)</span><br><span class="line">result = w1 @ v</span><br><span class="line"></span><br><span class="line">k_shift = k[[<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>],:] <span class="comment"># 你爱我</span></span><br><span class="line">v_shift = v[[<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>],:] <span class="comment"># 你爱我</span></span><br><span class="line">shift_w = q@k_shift.transpose(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">shift_w1 = F.softmax(shift_w, dim=<span class="number">1</span>)</span><br><span class="line">shift_result = shift_w1 @ v_shift</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(result, shift_result))</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>

<p>即’我’在 我爱你 和 你爱我 中的语义是一样的，这是肯定不对的。<strong>所以，就需要我们在transformer中添加位置编码，让模型在计算token之间的相似度时，能知晓两个token之间的相对距离</strong></p>
<h2 id="有几种位置编码"><a href="#有几种位置编码" class="headerlink" title="有几种位置编码"></a>有几种位置编码</h2><p>位置编码主要分为两种，第一种是想办法将位置信息融入到输入中，这是绝对位置编码的一般做法。另一种是修改一下Attention机制的结构，使其能够在计算注意力分数时，考虑到位置信息，这构成了相对位置编码的一般做法。我们首先介绍一下绝对位置编码。</p>
<h3 id="绝对位置编码"><a href="#绝对位置编码" class="headerlink" title="绝对位置编码"></a>绝对位置编码</h3><p>其公式如下：  </p>
<div style="text-align: center;">

<p><img src="/position_encoding.png" alt="attention"></p>
</div>

<p>其中pos代表单个token的位置索引，比如1000个token，这个pos的取值则为0-999。i表示同一个token的向量的某一个维度索引，其中偶数维都有sin函数计算，奇数维都由cos函数计算。其代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)  <span class="comment">## (max_len, 1)</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) * (-math.log(<span class="number">10000.0</span>) / d_model))  <span class="comment">## d_model/2 中结果</span></span><br><span class="line"></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)  <span class="comment">## 在第一个维度添加一个维度，便于批处理  (1, max_len, d_model)</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + torch.tensor(self.pe[:, :x.size(<span class="number">1</span>)], requires_grad=<span class="literal">False</span>)  <span class="comment">## max_len是模型能接受的最大长度token输入，推理时token长度是不定的，x.size()是B,T,C</span></span><br><span class="line">        <span class="comment">## x.size(1)是输入的token长度，则将输入token长度对应的位置编码加入到计算中</span></span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>

<p>为什么要使用这种sin和cos交替的形式来表示一个token的位置信息呢？transformer作者在论文中并没有明说。但是根据公式的形式，我们可以发现这个位置编码的几个性质：  </p>
<ul>
<li>有界性：sin函数和cos函数都是有界的，所以位置编码的值不会太大也不会太小，在-1到1的范围之内</li>
<li>周期性：不同token的同一个维度的位置编码值可以看作是同一个频率的正余弦向量，因为分母是一样大的，只是分子不同。即一个1000维的位置向量，其低维处的变量是高频频变化的，高维处的变量是低频变化的。周期性可以使得位置编码在处理较长的序列时，仍然能生成合理的值。</li>
<li>叠加性：对于同一个token而言，每两个不同的维度对，比如0-1和2-3，都是不同频率的正余弦向量在同一个点处的值。任意周期函数都可以用傅里叶展开公式变为三角函数的无穷级数。这里可以用这种方式理解，即叠加不同频率的正余弦函数，试图表示token的位置信息</li>
<li>能够反映一定的相对位置：PE(x+k)可以用PE(x)来线性表示。因为sin(x+k)&#x3D;sin(x)cosk + cos(x)sink，其中sink和cosk看作常数。即给定距离k和当前位置，k距离处的位置编码是当前位置关于距离k的线性组合</li>
<li>远程衰减：两个位置编码的点积取决于二者之间的相对位置，即两个位置编码的点积值可以反映其相对位置的大小。由下图可知，当相对位置增大时，点积的值是在减小的。而且这种点积具有对称性。</li>
<li>综上所述，绝对位置编码能够反映token之间的绝对位置信息，同时也能够一定程度上反映token之间的相对位置。</li>
</ul>
<div style="display: flex; justify-content: space-between;">
  <style>.zjnjcdirwmjn{width: 45%; height: auto;}</style><img src="/2025/04/02/transformer-PE/02.png" class="zjnjcdirwmjn" alt="attention">
  <style>.jjpfncxpabbw{width: 45%; height: auto;}</style><img src="/2025/04/02/transformer-PE/03.png" class="jjpfncxpabbw" alt="attention">
</div>
</article><!-- lincense--><div class="license-wrapper"><p> <span>Author:  </span><a href="http://example.com">武丢丢</a></p><p> <span>Link:  </span><a href="http://example.com/2025/04/02/transformer-PE/">http://example.com/2025/04/02/transformer-PE/</a></p><p> <span>Copyright:  </span><span>All articles in this blog are licensed under <a rel="license noopener" target="_blank" href="https://creativecommons.org/licenses/by-nc-nd/3.0">CC BY-NC-SA 3.0</a> unless stating additionally.</span></p></div><div class="post-paginator"><a class="nextSlogan" href="/2025/03/31/Linux%E5%AE%89%E8%A3%85comsol/" title="Linux安装comsol"><span>NextPost ></span><br><span class="nextTitle">Linux安装comsol</span></a><div class="clear"></div></div><div id="comment"></div></section></article><footer id="cxo-footer-outer"><div id="cxo-footer-inner"><p class="footer-container"><span>Site by </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span> | theme </span><a target="_blank" rel="noopener" href="https://github.com/Longlongyu/hexo-theme-Cxo"><span>Cxo</span></a></p><i class="fa fa-user"> </i><span id="busuanzi_value_site_uv"></span><span> | </span><i class="fa fa-eye"> </i><span id="busuanzi_value_site_pv"></span></div></footer><!-- catelog--><div class="toc-wrapper" style="top: 70vh;"><div class="toc-catalog"><i class="fa fa-list"> </i><span>CATALOG</span></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer%E4%B9%8B%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">1.</span> <span class="toc-text">Transformer之位置编码</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">1.1.</span> <span class="toc-text">为什么需要位置编码</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.1.</span> <span class="toc-text">词袋模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Attention%E6%9C%BA%E5%88%B6%E7%9A%84%E7%BC%BA%E9%99%B7"><span class="toc-number">1.1.2.</span> <span class="toc-text">Attention机制的缺陷</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%89%E5%87%A0%E7%A7%8D%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">1.2.</span> <span class="toc-text">有几种位置编码</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%9D%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">1.2.1.</span> <span class="toc-text">绝对位置编码</span></a></li></ol></li></ol></li></ol></div><!-- top--><i class="fa fa-arrow-up close" id="go-up" aria-hidden="true"></i></body></html>