<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><meta name="author" content="武丢丢"><meta name="renderer" content="webkit"><meta name="copyright" content="武丢丢"><meta name="keywords" content="武丢丢"><meta name="description" content="null"><meta name="Cache-Control" content="no-cache"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><title>Punchy's Blog</title><link rel="stylesheet" href="/css/style.css?v=2018.7.9"><link rel="stylesheet" href="/css/animation.css?v=2018.7.9"><link rel="icon" href="/img/assets/favicon.ico"><link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css?version=1.5.6"><!-- scripts--><script>(function( w ){
  "use strict";
  // rel=preload support test
  if( !w.loadCSS ){
    w.loadCSS = function(){};
  }
  // define on the loadCSS obj
  var rp = loadCSS.relpreload = {};
  // rel=preload feature support test
  // runs once and returns a function for compat purposes
  rp.support = (function(){
    var ret;
    try {
      ret = w.document.createElement( "link" ).relList.supports( "preload" );
    } catch (e) {
      ret = false;
    }
    return function(){
      return ret;
    };
  })();

  // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
  // then change that media back to its intended value on load
  rp.bindMediaToggle = function( link ){
    // remember existing media attr for ultimate state, or default to 'all'
    var finalMedia = link.media || "all";

    function enableStylesheet(){
      link.media = finalMedia;
    }

    // bind load handlers to enable media
    if( link.addEventListener ){
      link.addEventListener( "load", enableStylesheet );
    } else if( link.attachEvent ){
      link.attachEvent( "onload", enableStylesheet );
    }

    // Set rel and non-applicable media type to start an async request
    // note: timeout allows this to happen async to let rendering continue in IE
    setTimeout(function(){
      link.rel = "stylesheet";
      link.media = "only x";
    });
    // also enable media after 3 seconds,
    // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
    setTimeout( enableStylesheet, 3000 );
  };

  // loop through link elements in DOM
  rp.poly = function(){
    // double check this to prevent external calls from running
    if( rp.support() ){
      return;
    }
    var links = w.document.getElementsByTagName( "link" );
    for( var i = 0; i < links.length; i++ ){
      var link = links[ i ];
      // qualify links to those with rel=preload and as=style attrs
      if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
        // prevent rerunning on link
        link.setAttribute( "data-loadcss", true );
        // bind listeners to toggle media back
        rp.bindMediaToggle( link );
      }
    }
  };

  // if unsupported, run the polyfill
  if( !rp.support() ){
    // run once at least
    rp.poly();

    // rerun poly on an interval until onload
    var run = w.setInterval( rp.poly, 500 );
    if( w.addEventListener ){
      w.addEventListener( "load", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    } else if( w.attachEvent ){
      w.attachEvent( "onload", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    }
  }


  // commonjs
  if( typeof exports !== "undefined" ){
    exports.loadCSS = loadCSS;
  }
  else {
    w.loadCSS = loadCSS;
  }
}( typeof global !== "undefined" ? global : this ) );</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" defer></script><script src="/js/main.js?v=2018.7.9" defer></script><!-- fancybox--><link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script><!-- busuanzi--><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><meta name="generator" content="Hexo 7.1.1"></head><body><section class="profile-close" id="cxo-profile"><div class="profile-avatar"><i class="fa fa-caret-left"></i><img src="/img/assets/me.png"></div><!--.profile-saying
  i.fa.fa-comment
  .saying--><div class="cxo-profile-inner"><div class="profile-name">punchy</div><div class="profile-signature">Stay foolish Stay hungry</div><div class="read-progress"></div></div></section><header id="cxo-intro" style="height: 70vh;background-image: url(/img/intro/index-bg.png);"><nav id="cxo-intro-nav"><section><div class="intro-nav-title"><a href="/">Wudiudiu's Blog</a></div><div class="intro-nav-label-box"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div><i class="fa fa-bars intro-nav-menu"><div class="intro-nav-drop"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div></i><div class="clear"></div></section></nav><h1>Wudiudiu's Blog</h1><h5>Welcome to my Blog!</h5><div class="intro-social"></div></header><article class="cxo-up" id="cxo-content-outer"><section id="cxo-content-inner"><section id="recent-posts"><div class="recent-post-item"><div class="post-title"><a href="/2025/04/07/transformer-MLP/">transformer-MLP</a></div><div class="post-time"><i class="fa fa-table"></i><time datetime="2025-04-07T13:51:00.000Z">2025-04-07</time></div><div class="recent-post-content">
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    processEscapes: true,
  }
};



为什么会有MLP？transformer架构中，最主要的两个模块就是Attention-Layer和MLP-Layer。MLP全称为多层感知机，是神经网络的最主要的组成部分。其结构十分简单，主要由两层线性变换层和一个激活函数构成，其公式如下：
$$MLP &#x3D; \sigma(X \cdot W_1 + b_1) \cdot W_2 + b_2 \tag{1.1}$$
在神经网络中， ...</div><div class="recent-post-tags"><a class="post-tag fa fa-tag" href="javascript:void(0)" date-tags="LLM"> LLM</a></div><div class="read-more"><a class="more" href="/2025/04/07/transformer-MLP/#more">ReadMore</a></div><hr></div><div class="recent-post-item"><div class="post-title"><a href="/2025/04/07/RAG-langchain/">RAG-langchain</a></div><div class="post-time"><i class="fa fa-table"></i><time datetime="2025-04-07T08:27:32.000Z">2025-04-07</time></div><div class="recent-post-content">RAG学习数据加载我们需要利用给定的数据形成知识库，数据的类型多种多样，langchain提供了很多工具来帮助我们加载和处理数据。比如加载PDF文档。
从某一个目录下加载所有PDF文档123456from langchain_community.document_loaders import PyPDFDirectoryLoaderDATA_PATH = &quot;data&quot;document_loader = PyPDFDirectoryLoader(DATA_PATH)document = document_loader.load()
上面返回的document是一个列表，其中的 ...</div><div class="recent-post-tags"><a class="post-tag fa fa-tag" href="javascript:void(0)" date-tags="LLM"> LLM</a></div><div class="read-more"><a class="more" href="/2025/04/07/RAG-langchain/#more">ReadMore</a></div><hr></div><div class="recent-post-item"><div class="post-title"><a href="/2025/04/04/transformer-Normalization/">transformer-Normalization</a></div><div class="post-time"><i class="fa fa-table"></i><time datetime="2025-04-04T09:04:59.000Z">2025-04-04</time></div><div class="recent-post-content">
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    processEscapes: true,
  }
};



NormalizationTransformer学习笔记三：为什么Transformer要用LayerNorm&#x2F;Batch Normalization &amp; Layer Normalization （批量&amp;层标准化)
深入理解Batch Normalization原理与作用
Normalization又称为标准化或者规范化，是在深度学习中经常使用的一种算法。其核心思 ...</div><div class="recent-post-tags"><a class="post-tag fa fa-tag" href="javascript:void(0)" date-tags="LLM"> LLM</a></div><div class="read-more"><a class="more" href="/2025/04/04/transformer-Normalization/#more">ReadMore</a></div><hr></div><div class="recent-post-item"><div class="post-title"><a href="/2025/04/02/transformer-PE/">transformer-位置编码</a></div><div class="post-time"><i class="fa fa-table"></i><time datetime="2025-04-02T07:33:56.000Z">2025-04-02</time></div><div class="recent-post-content">
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    processEscapes: true,
  }
};



Transformer之位置编码为什么需要位置编码注意力计算公式如下图所示：  

  .pxkimmvffoud{width: 40%; height: 40%;}


其中q和k分别为query和key，代表token的语义信息。如果没有位置编码，那么q和k的计算结果是不受二者之间相对距离大小的影响的。也就是说，如果没有位置编码，那么transformer就是一个类词袋模型。
词袋模型 ...</div><div class="recent-post-tags"><a class="post-tag fa fa-tag" href="javascript:void(0)" date-tags="LLM"> LLM</a></div><div class="read-more"><a class="more" href="/2025/04/02/transformer-PE/#more">ReadMore</a></div><hr></div><div class="recent-post-item"><div class="post-title"><a href="/2025/03/31/Linux%E5%AE%89%E8%A3%85comsol/">Linux安装comsol</a></div><div class="post-time"><i class="fa fa-table"></i><time datetime="2025-03-31T02:50:03.000Z">2025-03-31</time></div><div class="recent-post-content">Linux安装comsol在linux系统中安装comsol和在win中有所不同，接下来从下载压缩包，解压，创建文件夹，挂载，安装等步骤讲解如何在linux系统中安装一个comsol。
下载iso文件首先到IT技术之家下载comsol的iso文件，版本是comsol6.3。下载地址下载好后需要上传到远程linux服务器，可以使用xftp或者ssh，如果你是直接在现场操作的话则不需要上传了，拷贝一下就行。注意如果U盘插入到电脑中但却点不开的话，可能需要先将U盘挂载到某一个文件夹下，这样才能将U盘中的文件取出
解压rar文件上面下载的压缩包是带密码的，建议使用unrar解压,sudo apt in ...</div><div class="recent-post-tags"><a class="post-tag fa fa-tag" href="javascript:void(0)" date-tags="Linux"> Linux</a></div><div class="read-more"><a class="more" href="/2025/03/31/Linux%E5%AE%89%E8%A3%85comsol/#more">ReadMore</a></div><hr></div><div class="recent-post-item"><div class="post-title"><a href="/2025/03/24/%E5%91%BD%E4%BB%A4%E5%92%8C%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/">命令和常见问题</a></div><div class="post-time"><i class="fa fa-table"></i><time datetime="2025-03-24T09:56:01.000Z">2025-03-24</time></div><div class="recent-post-content">ssh连接远程主机
ssh -p 22 root@host
其中-p后面是端口号，root是远程主机上的用户名，host是远程主机的ip地址。以组里的服务器为例，则为：ssh -p 22 123@10.181.83.26

如果连接不上
检查是否启动：sudo systemctl status sshd如果显示activate，则表明ssh服务已经启动
启动命令：sudo systemctl start sshd
设置开机自启动命令:sudo systemctl enable sshd
也有可能时防火墙不允许指定端口通过，检查防火墙规则：sudo ufw status
允许端口22通过：sud ...</div><div class="read-more"><a class="more" href="/2025/03/24/%E5%91%BD%E4%BB%A4%E5%92%8C%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/#more">ReadMore</a></div><hr></div><div class="recent-post-item"><div class="post-title"><a href="/2024/08/04/GPT2%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/">GPT2环境配置</a></div><div class="post-time"><i class="fa fa-table"></i><time datetime="2024-08-04T09:28:27.000Z">2024-08-04</time></div><div class="recent-post-content">环境配置记录本地配置方法：去pytorch官网找到要下载的历代版本，比如本次配置版本为		conda install pytorch&#x3D;&#x3D;2.2.0 torchvision&#x3D;&#x3D;0.17.0 torchaudio&#x3D;&#x3D;2.2.0 pytorch-cuda&#x3D;11.8 -c pytorch -c nvidia直接使用这个命令即可下载pytoch相关包以及cuda包，此处cuda包的版本不需要非得和电脑上安装的cuda软件版本一致。最后在下载一个cudnn的包即可
速度太慢直接使用上述命令的速度太慢，故采用本地下载方式。
1.去清华镜像 ...</div><div class="recent-post-tags"><a class="post-tag fa fa-tag" href="javascript:void(0)" date-tags="深度学习环境配置"> 深度学习环境配置</a></div><div class="read-more"><a class="more" href="/2024/08/04/GPT2%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/#more">ReadMore</a></div><hr></div><div class="recent-post-item"><div class="post-title"><a href="/2024/07/26/%E5%B7%A5%E7%A8%8B%E6%8E%A8%E9%80%81%E8%87%B3github%E6%B5%81%E7%A8%8B/">工程推送至github流程</a></div><div class="post-time"><i class="fa fa-table"></i><time datetime="2024-07-26T09:42:42.000Z">2024-07-26</time></div><div class="recent-post-content">步骤：1.github创建一个与工程名同名的仓库，不要勾选任何可选项
2.进入工程名目录
3.右键打开git bash
4.运行以下代码：
git init
touch README.md
git add README.md
git commit -m &quot;first commit&quot;
git branch -M main
git remote add origin git@github.com:wu-diu-diu/仓库名.git
git push -u origin main

main可以换成master
本步骤适用于将一个完善了的project上传至github的情况
</div><div class="recent-post-tags"><a class="post-tag fa fa-tag" href="javascript:void(0)" date-tags="github学习"> github学习</a></div><div class="read-more"><a class="more" href="/2024/07/26/%E5%B7%A5%E7%A8%8B%E6%8E%A8%E9%80%81%E8%87%B3github%E6%B5%81%E7%A8%8B/#more">ReadMore</a></div><hr></div><div class="recent-post-item"><div class="post-title"><a href="/2024/07/25/np-tensordot/">np.tensordot</a></div><div class="post-time"><i class="fa fa-table"></i><time datetime="2024-07-25T03:00:45.000Z">2024-07-25</time></div><div class="recent-post-content">tensordot使用方法主要是对张量进行点乘，对于多维张量来说，有时候需要指定维度进行点乘。numpy.tensordot(a, b, axes=0,1,2)axes&#x3D;1的话，则结果是两个数组以矩阵相乘的形式计算.实际为a的最后一个维度和b的第一个维度进行点乘
若a，b是二维数组，此时即为矩阵乘法。numpy中的轴分布如下图：
axis&#x3D;0和axis&#x3D;2的情况参考numpy的官方解释
对于多维数组，axis可以指定多个轴进行点乘，例如（3， 4， 5）数组和（5， 4， 2）数组，可以指定第一个数组的1， 2轴和第二个数组的1， 0轴相点乘。代码为：
np.ra ...</div><div class="recent-post-tags"><a class="post-tag fa fa-tag" href="javascript:void(0)" date-tags="Numpy"> Numpy</a></div><div class="read-more"><a class="more" href="/2024/07/25/np-tensordot/#more">ReadMore</a></div><hr></div><div class="recent-post-item"><div class="post-title"><a href="/2024/07/23/dezerobug%E8%AE%B0%E5%BD%95/">dezerobug记录</a></div><div class="post-time"><i class="fa fa-table"></i><time datetime="2024-07-23T13:09:48.000Z">2024-07-23</time></div><div class="recent-post-content">Dezero Bug 记录在步骤52的时候，添加cupy库，引起一系列bug。1.UserWarning: Failed to auto-detect cl.exe path: &lt;class &#39;distutils.errors.DistutilsPlatformError&#39;&gt;: No Microsoft Visual C++ version found即cupy库需要使用编译器构建C&#x2F;C++扩展模块。在 Windows 系统上，通常使用 Microsoft Visual C++ 编译器（cl.exe）。如果你的系统上没有安装适当版本的 Visual C++ ...</div><div class="recent-post-tags"><a class="post-tag fa fa-tag" href="javascript:void(0)" date-tags="Dezero"> Dezero</a></div><div class="read-more"><a class="more" href="/2024/07/23/dezerobug%E8%AE%B0%E5%BD%95/#more">ReadMore</a></div><hr></div></section><nav class="cxo-page-nav"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">NEXT &#62;</a></nav></section></article><footer id="cxo-footer-outer"><div id="cxo-footer-inner"><p class="footer-container"><span>Site by </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span> | theme </span><a target="_blank" rel="noopener" href="https://github.com/Longlongyu/hexo-theme-Cxo"><span>Cxo</span></a></p><i class="fa fa-user"> </i><span id="busuanzi_value_site_uv"></span><span> | </span><i class="fa fa-eye"> </i><span id="busuanzi_value_site_pv"></span></div></footer><!-- catelog--><!-- top--><i class="fa fa-arrow-up close" id="go-up" aria-hidden="true"></i></body></html>